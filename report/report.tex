\documentclass[10pt,conference]{IEEEtran}

% Font
\usepackage{lmodern}
%\usepackage[T1]{fontenc}

\usepackage{bbm}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment
\usepackage{makecell}	% For multiline cells in tables
% \usepackage{subfig}
\usepackage{subcaption}

% For the figures
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{changepage}
\usepackage{tikz}
\usepackage{pgf}
\usetikzlibrary{graphs, shapes}

% justify text in bibliography
\usepackage{ragged2e}  % for \justifying
\usepackage{etoolbox}
\apptocmd{\thebibliography}{\justifying}{}{}
% links in blue
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,citecolor=blue,linkcolor=,urlcolor=links}


\newcommand\tl[1]{\{\text{Label} = #1\}}
\newcommand\pl[1]{\{\text{Pred} = #1\}}
\newcommand\tp{\{\text{Pred} = 1 \cap \text{Label} = 1\}}
\newcommand\fp{\{\text{Pred} = 1 \cap \text{Label} = 0\}}
\newcommand\tn{\{\text{Pred} = 0 \cap \text{Label} = 0\}}
\newcommand\fn{\{\text{Pred} = 0 \cap \text{Label} = 1\}}


\begin{document}
\title{{\LARGE Automatic detection of available area for rooftop solar panel installations}\vspace{-3mm}}    

\author{
  Alexander \textsc{Apostolov} (\texttt{alexander.apostolov@epfl.ch})\\
  Auguste \textsc{Baum} (\texttt{auguste.baum@epfl.ch})\\
  Ghali \textsc{Chraibi} (\texttt{ghali.chraibi@epfl.ch})\\
  \\
  Supervisor: Roberto \textsc{Castello} (\texttt{roberto.castello@epfl.ch})\\ EPFL Laboratory of Solar Energy and Building Physics\\
  \\
  \textit{CS-433 Machine Learning --- December 2020, EPFL, Switzerland}
}
\maketitle

\begin{abstract}
%TODO explain why it is important an that we are the first ones to do it
%Say we also labelled
  In this report we propose and analyze a neural network for automatic detection of available rooftop solar panel installations on aerial images. We focus on tuning the network and on analyzing methods to use its outcome to make decisions. 
\end{abstract}

\section{Introduction}
%TODO change name
\subsection{Importance of the task}

\subsection{Data}
The dataset used consists of ortho-rectified [**TODO should explain ?**] images of Geneva canton, Switzerland provided by the Swiss Federal Office of Topography. The images are split in tiles corresponding to different region of the canton. Images are $250 \times 250$ pixels, RGB arrays saved in png format. Each pixel correspond to $0.25 \times 0.25 \text{ m}^2$.


\section{Models and Methods}

\subsection{Data labelling}
%Updated tool labelled images ourselves, show strange example
We labelled X images using an updated version of the labelling tool based on OpenCV provided by the Solar Energy and Building Physics Laboratory [**TODO should we cite like this ? **]. Each pixel that belongs to a rooftop where there is available space to place solar photovoltaic panels are classified as \emph{PV pixels}, whereas the other pixels are classified as \emph{no-PV pixels}, they do not correspond to rooftop or there are obstacles that do not allow to place any photovoltaic panels on it (e.g. chimney, pipe, windows, etc.).

The labelled images were chosen from a randomized subset of the dataset mentioned above, taken from different tiles (area) of the canton in order to have the most representative variety of rooftop shapes and types (industrial, old town, center town, countryside, etc.). 

\subsection{Data preprocessing}

\subsubsection{PV images vs. no-PV images}
In this paper, we distinguish PV images that contain at least some PV pixels from no-PV images that contain only no-PV pixels. We separate images in different folders as we want to manipulate precisely with how much no-PV images we train our model. We suppose it can have an impact for the robustness of our model, reinforcing the rejection of false positives.  

\subsubsection{Data augmentation}
As we donâ€™t have a lot of samples, we apply a random transformation on each sample (image and label) to artificially enlarge our dataset. The transformation is composed of :  
\begin{itemize}
	\item a random square crop which takes at least 60\% of the sample and is then resized to the size of the original sample
	\item a horizontal flip of the sample which happens with probability 0.5
\end{itemize}
  
\subsubsection{Training, validation, test split}
We split the dataset into a train, validation and test set in the following proportion 70\%/15\%/15\%.

The transformation mentioned above is only applied to the training set as we want to test our models on the original distribution. For the same reason, we use the original ratio of PV images / no-PV images for the validation set and the test set and alter this ration only when training the model.

\subsection{U-Net}
Computer vision tasks are commonly tackled using convolutional networks. 
Here we propose to use a special convolutional network called a \textbf{U-Net},~\cite{ronneberger2015unet}
introduced in 2015 for image segmentation in biomedical imaging.
We choose to use this model as it can yield state-of-the-art results with only few images.
The U-Net as shown in \autoref{fig:UNetarchitecture} consists of two parts,
the contracting and the expanding path.
% Pas sur de comprendre cette phrase
The former is used to \textit{detect features} on an image
and the latter to \textit{find the locality} of these features in the original space.

The contracting path we use consists of 5 stages where we apply two $3 \times 3$ convolutions padded by 1 pixel on each side,
each followed by a batch normalization layer and a rectified linear unit.
% Beaucoup de fois "stage"
The number of channels per stage after the first convolution of the stage remains constant throughout the stage.
We use the same number of channels as described in \autoref{fig:UNetarchitecture}, namely 64, 128, 256, 512 and 1024 for each stage of the contracting path. 
Stages in the contracting path are separated by a $2 \times 2$ max-pooling layer with a stride of 2. 

The expanding path we use consists of 4 stages starting by an upsampling by a factor of 2 and a $2 \times 2$ convolution to halve the number of channels,
a copy of the feature map in the corresponding stage of the contracting path is concatenated and then $3 \times 3$ convolutions padded by 1 pixel on each side are applied,
each followed by a batch normalization layer and a rectified linear unit.
The number of channels per stage remains constant after the second convolution at any given stage.
We use the same number of channels per stage as in \autoref{fig:UNetarchitecture}, namely 512, 256, 128 and 64.

The model terminates by a $1 \times 1$ convolution to a feature map with only one channel.
From this output we can compute, for each pixel, the probability that it represents an area that is available for a solar panel.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=\columnwidth]{report/images/UNet.png}
    \caption{
        U-Net example as described in the original paper~\cite{ronneberger2015unet}.
        Each blue box corresponds to a multi-channel feature map; the number of channels is found on top of the box, and the dimensions are at the lower left edge of the box.
        White boxes represent copied feature maps.
        The arrows denote the different operations.
    }
    \label{fig:UNetarchitecture}
\end{figure}

\subsection{Loss}\label{loss}
Our model has to classify each pixel into 2 classes, either the area is available for a rooftop solar panel or not. We try the following three losses:
\begin{itemize}
    \item Binary cross entropy (BCE)
    \item Weighted binary cross entropy (wBCE)
    \item L1 loss
\end{itemize}
Because the U-Net outputs an arbitrary value in $\mathbb{R}$ for each pixel, we apply a sigmoid layer before computing the loss.

The weighted binary cross entropy can be used when there is a significant class imbalance, which is indeed the case here:
most pixels are not available for rooftop solar panels. 
The formula for this loss is:
\begin{equation*}
    %L = \frac{1}{N} \sum_{n=1}^N l_n, \\
    L = \frac{1}{N} \sum_{n=1}^N  [py_n \log(\sigma(x_n))
        + (1-y_n) \log(1-\sigma(x_n))],
    %l_n = - [py_n \log(\sigma(x_n))
        %+ (1-y_n) \log(1-\sigma(x_n))]
\end{equation*}
where $x_n$ and $y_n$ are respectively the output of the model and the true class (0 or 1) for pixel $n$,
and $p$ is the weight given to the positive class. 
Setting $p>1$ increases the recall, whereas $p<1$ increases the precision.
As described in the documentation of PyTorch we set this weight as
% $\frac{\text{\#negative pixels}}{\text{\#positive pixels}}$
$\frac{\mathrm{support}(0)}{\mathrm{support}(1)}$
in the dataset.


\subsection{Training}
We train the model with different parameters and choose the best combination which has the best results on the validation set. We vary the following parameters:

\subsubsection{Optimizer}
We decide to try Adam~\cite{kingma2014adam} and Stochastic Gradient Descent (SGD) to train the U-Net.

\subsubsection{Effect of using noPV images during training}
We try using different amounts of noPV images during training. We think that using too little might make for a model which doesn't generalize well, while using too much might bias the model. We measure the amount of noPV images by considering the percentage of noPV images compared to PV images in the train set. The percentageswe consider are $0\%, 25\% \text{ and } 50\%$.

\subsubsection{Loss}
We use the three losses mentioned in \autoref{loss}. For the weighted cross entropy loss we use a different weight based on the percentage of noPV images we use in the train set. We compute the following weight using the formula from \autoref{loss}.
\begin{center}
    \begin{tabular}{||c | c||} 
        \hline
        Percentage noPV & Weight for wBCE\\ [0.5ex] 
        \hline\hline
        $0\%$ & $5.13$ \\
        \hline
        $25\%$ & $6.46$ \\
        \hline
        $50\%$ & $8.10$ \\
       % [1ex] 
        \hline
    \end{tabular}
\end{center}

\subsubsection{Learning rate scheduling}
We try different constant values of the learning rate of the optimizer. However, small values tend to reach a better minimum of the loss function but very slowly, while big values are faster but don't guarantee a good convergence. We propose to use a learning rate scheduler to combine the best of both. We start training with a larger learning rate and decrease it during training.

\subsection{Tuning threshold on probability after training model}
Since the model outputs weights (real numbers in all of $\mathbb{R}$),
we use a sigmoid to transform the weights into numbers in $[0,1]$.
Once this is done, we still need to find the best decision boundary---the threshold probability $\theta$ over which a pixel is decided to be available for PV.
For that we perform a grid-search over thresholds, computing the $F_1$-score over our validation set
for each threshold.
Then, we compute order statistics (first, second and third quartiles) and find the threshold that maximizes the following metric:
\begin{equation} \label{eq:threshold_metric}
    f(\theta) = Q_2(\theta) - (Q_3(\theta) - Q_1(\theta))
\end{equation}
where $Q_i$ is the $i$\textsuperscript{th} quartile.
Our reasoning is that while the median $F_1$-score
should be high, we should also prioritize
values for which we are more certain of the ``true'' $F_1$-score.

Once the best threshold is found, we use this to produce
the final model; we apply this on our test set and compute
standard classification metrics for each prediction.


As a reminder, the precision of the prediction is computed as
\begin{equation*}
    \text{precision} = \frac{\#\tp}{\#\tl1}
\end{equation*}
and the recall as
\begin{equation*}
    \text{recall} = \frac{\#\tp}{\#\pl1},
\end{equation*}
where $\tl{i}$ corresponds to the set of pixels classified as $i$ is the ground-truth label and 
$\pl{i}$ corresponds to the set of pixels classified as 1 is the predicted label.

The $F_1$-score is computed as 
\begin{equation*}
    F_1 = 2 \times \frac{\text{precision} \times \text{recall}}{
\text{precision} + \text{recall}}
\end{equation*}

Additionally, we compute the accuracy as
\begin{align*}
    \text{accuracy} = \\
    \frac{\#\tp + \#\tn}{\#\{\text{All}\}}
\end{align*}
and the Jaccard loss or intersection-over-union (IoU) as
\begin{equation*}
    \text{IoU} = \frac{\#\tp}{\#\{\text{Pred} = 1 \cup \text{Label} = 1\}}.
\end{equation*}

The way we treat images that are noPV is quite important for
measuring the performance of a model.
Indeed, a number of our models have been trained with few or no
noPV images, whereas the validation and test sets are a more
faithful representation of the data that would be passed to a
production-grade model.

%The size of the validation and test sets is according to the train-validation-test split decided in the
%beginning, namely 15\% of the total dataset for each.

EXPLAIN HOW THE noPV ARE TREATED (set to 0? set to 1? discarded?)
Set to 1 

\section{Results}
After selecting appropriate learning rates we choose to train the following models:

\begin{center}
    \begin{tabular}{||c | c  | c | c | c ||} 
        \hline
        $N^o$ & Optimizer & $\%$noPV & Learning rate & Loss \\ [0.5ex] 
        \hline\hline
        1 & ADAM & $0\% $ & $10^{-3}$ & wBCE \\
        \hline
        2 & ADAM & $0\% $ & $10^{-4}$ & wBCE \\
        \hline
        3 & ADAM & $0\% $ & $10^{-3}$ and $10^{-4}$ & wBCE \\
        \hline
        4 & ADAM & $25\% $ & $10^{-3}$  & wBCE \\
        \hline
        5 & ADAM & $25\% $ & $10^{-4}$ & wBCE \\
        \hline
        6 & ADAM & $25\% $ & $10^{-3}$ and $10^{-4}$ & wBCE \\
        \hline
        7 & ADAM & $50\% $ & $10^{-3}$ & wBCE \\
        \hline
        8 & ADAM & $50\% $ & $10^{-4}$ & wBCE \\
        \hline
        9 & ADAM & $50\% $ & $10^{-3}$ and $10^{-4}$ & wBCE \\
        \hline
        10 & SGD & $0\% $ & $10^{-3}$ & wBCE \\
        \hline
        11 & SGD & $0\% $ & $10^{-2}$ & wBCE \\
        \hline
        12 & SGD & $0\% $ & $10^{-2}$ and $10^{-3}$ & wBCE \\
        \hline
        13 & SGD & $25\% $ & $10^{-3}$ & wBCE \\
        \hline
        14 & SGD & $25\% $ & $10^{-2}$ & wBCE \\
        \hline
        15 & SGD & $25\% $ & $10^{-2}$ then $10^{-3}$ & wBCE \\
        \hline
        16 & SGD & $50\% $ & $10^{-3}$ & wBCE \\
        \hline
        17 & SGD & $50\% $ & $10^{-2}$ & wBCE \\
        \hline
        18 & SGD & $50\% $ & $10^{-2}$ and $10^{-3}$ & wBCE \\
        \hline
        19 & ADAM & $0\% $ & $10^{-3}$ and $10^{-4}$ & BCE \\
        \hline
        20 & ADAM & $25\% $ & $10^{-3}$ and $10^{-4}$ & BCE \\
        \hline
        21 & ADAM & $50\% $ & $10^{-3}$ and $10^{-4}$ & BCE \\
        \hline
        22 & ADAM & $0\% $ & $10^{-3}$ and $10^{-4}$ & L1 \\
        \hline
        23 & ADAM & $25\% $ & $10^{-3}$ and $10^{-4}$ & L1 \\
        \hline
        24 & ADAM & $50\% $ & $10^{-3}$ and $10^{-4}$ & L1 \\
        \hline
        
       % [1ex] 
        \hline
    \end{tabular}
\end{center}
We write two values for the learning rate, when we use learning rate rescheduling. 

We notice that rescheduling the learning rate during the training only once after 50 epochs gives the best results.
We also notice that when rescheduling, we need to shorten the training to 80 epochs to avoid overfitting,
whereas with a constant learning rate we train for 100 epochs to have the best results before overfitting.

% Better in methods or results? 
Each model is run on a validation set to perform a grid-search of the best threshold,
and the model is then applied with that threshold to a test set.
Various standard classification metrics are computed and collected in \autoref{tbl:test_results}.

% I also want to put the 1st and 3rd quartiles -- maybe I could have it span the 2 columns?
\begin{table*}[!ht]
    \begin{center}
        \begin{tabular}{||c | c c c c c c||} 
             \hline
             \input{report/res_latex}
        \end{tabular}
    \end{center}
    \caption{Test results for each model.
        The threshold is $\arg\max f(\theta)$ where $f$ is defined in \autoref{eq:threshold_metric}.
    }
    \label{tbl:test_results}
\end{table*}

 


\section{Discussion}
Discussion is done here.

\section{Conclusion}
Conclusion is done here.

\section*{Acknowledgements}

\bibliographystyle{ieeetr}
\bibliography{report}


\end{document}
