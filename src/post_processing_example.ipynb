{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn.metrics\n",
    "\n",
    "from model.unet_model import UNet\n",
    "from visualisation import plot_precision_recall_f1, show_full_comparisonTestGenerator\n",
    "\n",
    "from load_data import *\n",
    "from post_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the metrics available for testing.\n",
    "METRICS = {\n",
    "    \"accuracy\": sklearn.metrics.accuracy_score,\n",
    "    \"balanced_accuracy\": sklearn.metrics.balanced_accuracy_score,\n",
    "    \"average_precision\": sklearn.metrics.average_precision_score,\n",
    "    \"neg_brier_score\": sklearn.metrics.brier_score_loss,\n",
    "    \"f1\": sklearn.metrics.f1_score,\n",
    "    \"f1_weighted\": lambda true, pred: sklearn.metrics.f1_score(\n",
    "        true, pred, zero_division=1, average=\"weighted\"\n",
    "    ),\n",
    "    \"neg_log_loss\": sklearn.metrics.log_loss,\n",
    "    \"precision\": sklearn.metrics.precision_score,\n",
    "    \"precision_weighted\": lambda true, pred: sklearn.metrics.precision_score(\n",
    "        true, pred, zero_division=1, average=\"weighted\"\n",
    "    ),\n",
    "    \"recall\": sklearn.metrics.recall_score,\n",
    "    \"recall_weighted\": lambda true, pred: sklearn.metrics.recall_score(\n",
    "        true, pred, zero_division=1, average=\"weighted\"\n",
    "    ),\n",
    "    \"jaccard\": sklearn.metrics.jaccard_score,\n",
    "    \"jaccard_weighted\": lambda true, pred: sklearn.metrics.jaccard_score(\n",
    "        true, pred, average=\"weighted\"\n",
    "    ),\n",
    "    \"roc_auc\": sklearn.metrics.roc_auc_score,\n",
    "    \"roc_auc_ovr\" : sklearn.metrics.roc_auc_score,\n",
    "    \"roc_auc_ovo\" : sklearn.metrics.roc_auc_score,\n",
    "    \"roc_auc_ovr_weighted\" : sklearn.metrics.roc_auc_score,\n",
    "    \"roc_auc_ovo_weighted\" : sklearn.metrics.roc_auc_score,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    model_name,\n",
    "    from_file=True,\n",
    "    to_file=False,\n",
    "    validation=True,\n",
    "    test=[\"precision\", \"recall\", \"f1\", \"accuracy\", \"jaccard\"],\n",
    "    concat=False,\n",
    "    plot=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    ========\n",
    "    model_name : str\n",
    "        Which model to do things with. This is assumed to be both the name of the directory in which parameters are stored, and the name of the parameters file.\n",
    "    from_file : bool\n",
    "        Whether data should be loaded from a file; otherwise it will be generated.\n",
    "        The file should:\n",
    "            - be in the same directory as the model parameters\n",
    "            - be called \"data.npz\"\n",
    "            - contain 4 arrays: \"val_predictions\", \"val_labels\", \"test_predictions\" and \"test_labels\".\n",
    "        Labels are expected to be floats and will be thresholded.\n",
    "        Predictions are expected to be raw (not probabilities).\n",
    "    to_file : bool\n",
    "        If data is generated (not loaded from a file), whether to save to a file in the model directory, according to the form described in from_file.\n",
    "        Irrelevant when from_file is set to True.\n",
    "    validation : bool\n",
    "        Whether to go through validation steps (to find the best threshold).\n",
    "    test : list of str\n",
    "        Which metrics to use for testing (evaluate the model with a given threshold).\n",
    "        The results are stored in a txt file called \"test_results.txt\" in the model directory.\n",
    "        If empty then testing is skipped.\n",
    "    concat : bool\n",
    "        During testing, whether to compute each metric once, on the concatenation of the whole test set.\n",
    "    plot : bool\n",
    "        Whether to show plots during run.\n",
    "    \"\"\"\n",
    "    model_dir = os.path.join(dir_models, model_name)\n",
    "    params_file = os.path.join(model_dir, model_name)\n",
    "    new_section = \"=\" * 50\n",
    "\n",
    "    print(\"Importing model parameters from {}\".format(params_file))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(params_file, map_location=torch.device(\"cpu\")))\n",
    "\n",
    "    # File where data are stored, or will be if there aren't already\n",
    "    data_file = os.path.join(model_dir, \"data.npz\")\n",
    "    print(new_section)\n",
    "    if from_file:\n",
    "        print(\"Loading data\")\n",
    "        arrays = np.load(data_file)\n",
    "        val_predictions, val_labels, test_predictions, test_labels = arrays.values()\n",
    "    else:\n",
    "        print(\"Generating data\")\n",
    "        dir_data_validation = os.path.join(dir_data, \"validation\")\n",
    "        dir_data_test = os.path.join(dir_data, \"test\")\n",
    "\n",
    "        _, validation_dl, test_dl = load_data(\n",
    "            dir_data_validation=dir_data_validation,\n",
    "            dir_data_test=dir_data_test,\n",
    "            prop_noPV_training=0,  # Has no impact\n",
    "            min_rescale_images=0,  # Has no impact\n",
    "            batch_size=100,  # All of them\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get images and labels from both DataLoaders\n",
    "            val_images, val_labels = next(iter(validation_dl))\n",
    "            test_images, test_labels = next(iter(test_dl))\n",
    "            val_images = val_images.to(device, dtype=torch.float32)\n",
    "            test_images = test_images.to(device, dtype=torch.float32)\n",
    "            # Make predictions (predictions are not probabilities at this stage)\n",
    "            print(\"Running model on data\")\n",
    "            val_predictions = model(val_images)\n",
    "            test_predictions = model(test_images)\n",
    "            # Convert to numpy arrays for computing\n",
    "            val_predictions = np.squeeze(val_predictions.cpu().numpy())\n",
    "            val_labels = np.squeeze(val_labels.cpu().numpy())\n",
    "            test_predictions = np.squeeze(test_predictions.cpu().numpy())\n",
    "            test_labels = np.squeeze(test_labels.cpu().numpy())\n",
    "            # Save to file as numpy arrays\n",
    "            if to_file:\n",
    "                print(\"Saving results to file\")\n",
    "                np.savez_compressed(\n",
    "                    data_file,\n",
    "                    val_predictions=val_predictions,\n",
    "                    val_labels=val_labels,\n",
    "                    test_predictions=test_predictions,\n",
    "                    test_labels=test_labels,\n",
    "                )\n",
    "\n",
    "    threshold_true = 0.5\n",
    "    val_labels = np.where(val_labels > threshold_true, 1, 0)\n",
    "    test_labels = np.where(test_labels > threshold_true, 1, 0)\n",
    "\n",
    "    if validation:\n",
    "        print(new_section)\n",
    "        print(\"Validation starting\")\n",
    "        _, _, _, best_threshold = find_best_threshold(\n",
    "            val_predictions, val_labels, n_thresholds=101, concat=concat, plot=plot\n",
    "        )\n",
    "        print(f\"Found best threshold to be {best_threshold:.4f}\")\n",
    "\n",
    "    if test:\n",
    "        print(new_section)\n",
    "        print(\"Testing starting with metrics:\")\n",
    "        print(\", \".join(test))\n",
    "        results = test_model(\n",
    "            test_predictions, test_labels, best_threshold, concat, *test\n",
    "        )\n",
    "        print(results)\n",
    "        summary_type = \"median\"\n",
    "        results_file = os.path.join(\n",
    "            model_dir, \"test_{}results.txt\".format(\"concat_\" if concat else \"\")\n",
    "        )\n",
    "        if concat:\n",
    "            results_summary = np.transpose(results)\n",
    "            print(\"Results:\")\n",
    "        else:\n",
    "            results_summary = np.transpose(summary_stats(results, type=summary_type))\n",
    "            print(f\"Summary statistics are based on the {summary_type}\")\n",
    "            print(\"Results (lower, mid-point, upper):\")\n",
    "        print(f\"\\tBest threshold = {best_threshold:.4f}\")\n",
    "        for i, measure in enumerate(test):\n",
    "            print(\"\\t{}: {}\".format(measure, results_summary[i, :]))\n",
    "        print(new_section)\n",
    "        print(\"Saving results to {}\".format(results_file))\n",
    "        np.savetxt(\n",
    "            results_file,\n",
    "            results_summary,\n",
    "            fmt=\"%.4f\",\n",
    "            delimiter=\" \",\n",
    "            header=f\"Threshold: {best_threshold:.3f}\\n{'  '.join(test)}\",\n",
    "        )\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\"../saved_models/6Adam_e_3_25noPV_BCEwithweights_epochs_80_schedulere_4_at50\"))\n",
    "# Visualise predictions of the model on individual images    \n",
    "vizualization_generator = show_full_comparisonTestGenerator(model,threshold_prediction=0.79)\n",
    "SHOW_N_IMAGES = 15\n",
    "count = 0\n",
    "for _ in vizualization_generator:\n",
    "    count +=1\n",
    "    if count >= SHOW_N_IMAGES:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
