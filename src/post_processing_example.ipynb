{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we go through our post-processing steps. We will decide on a model to use, then:\n",
    "1. - Apply the model to our validation and test sets, or\n",
    "   -  Load the same pre-generated data from a file\n",
    "2. Find the best threshold using the validation set\n",
    "3. Test the model at its best using various metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magics for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn.metrics\n",
    "\n",
    "from model.unet_model import UNet\n",
    "from visualisation import plot_precision_recall_f1, show_full_comparisonTestGenerator\n",
    "\n",
    "from load_data import *\n",
    "from post_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "dir_models = \"../saved_models\"\n",
    "model_name = \"21Adam_e_3_50noPV_BCEwithweights_epochs_80_schedulere_4_at50\"\n",
    "# Number of thresholds to try when `validation` is set\n",
    "n_thresholds = 101\n",
    "# Compute the metrics on the concatenation of all images in the set, instead of doing it for each image individually\n",
    "concat = True\n",
    "# Whether to plot a precision-recall curve and the F1-score during the threshold search\n",
    "plot = True\n",
    "# Load data from a file, don't generate it again\n",
    "from_file = True\n",
    "# Save data to a file (irrelevant if from_file is set to True)\n",
    "to_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(dir_models, model_name)\n",
    "params_file = os.path.join(model_dir, model_name)\n",
    "# All the metrics available for testing.\n",
    "METRICS = get_metrics()\n",
    "new_section = \"=\" * 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Which metrics to apply during test phase\n",
    "test = [\"precision\", \"recall\", \"f1\", \"jaccard\"]\n",
    "for metric in test:\n",
    "    if metric not in METRICS.keys():\n",
    "        raise ValueError(f\"{metric} is not a valid metric. Valid metrics are:\\n{list(METRICS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Getting data__\n",
    "\n",
    "If `from_file` is set we load data from a file specified in `data_file`.\n",
    "Otherwise we use the parameters to produce that data, and save it to `data_file` if `to_file` is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File where data are stored, or will be if they aren't already\n",
    "data_file_name = \"data.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join(model_dir, data_file_name)\n",
    "print(new_section)\n",
    "if from_file:\n",
    "    print(\"Loading data\")\n",
    "    arrays = np.load(data_file)\n",
    "    val_predictions, val_labels, test_predictions, test_labels = arrays.values()\n",
    "else:\n",
    "    print(\"Loading model\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(model_dir))\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Generating data\")\n",
    "    dir_data_validation = os.path.join(dir_data, \"validation\")\n",
    "    dir_data_test = os.path.join(dir_data, \"test\")\n",
    "\n",
    "    _, validation_dl, test_dl = load_data(\n",
    "        dir_data_validation=dir_data_validation,\n",
    "        dir_data_test=dir_data_test,\n",
    "        prop_noPV_training=0,  # Has no impact\n",
    "        min_rescale_images=0,  # Has no impact\n",
    "        batch_size=100,  # All of them\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        # Get images and labels from both DataLoaders\n",
    "        val_images, val_labels = next(iter(validation_dl))\n",
    "        test_images, test_labels = next(iter(test_dl))\n",
    "        val_images = val_images.to(device, dtype=torch.float32)\n",
    "        test_images = test_images.to(device, dtype=torch.float32)\n",
    "        # Make predictions (predictions are not probabilities at this stage)\n",
    "        print(\"Running model on data\")\n",
    "        val_predictions = model(val_images)\n",
    "        test_predictions = model(test_images)\n",
    "        # Convert to numpy arrays for computing\n",
    "        val_predictions = np.squeeze(val_predictions.cpu().numpy())\n",
    "        val_labels = np.squeeze(val_labels.cpu().numpy())\n",
    "        test_predictions = np.squeeze(test_predictions.cpu().numpy())\n",
    "        test_labels = np.squeeze(test_labels.cpu().numpy())\n",
    "        # Save to file as numpy arrays to avoid having to run again (the images will always be the same)\n",
    "        if to_file:\n",
    "            print(\"Saving results to file\")\n",
    "            np.savez_compressed(\n",
    "                data_file,\n",
    "                val_predictions=val_predictions,\n",
    "                val_labels=val_labels,\n",
    "                test_predictions=test_predictions,\n",
    "                test_labels=test_labels,\n",
    "            )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Finding the best threshold: we use the validation set to compute precision, recall and the F1-score.__\n",
    "\n",
    "We define the \"best\" threshold to be the one that\n",
    "maximises the F1-score if `concat` is set, and the median F1-score otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_section)\n",
    "print(\"Validation starting\")\n",
    "_, _, _, best_threshold = find_best_threshold(\n",
    "    val_predictions, val_labels, n_thresholds, concat=concat, plot=plot\n",
    ")\n",
    "print(f\"Found best threshold to be {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Testing the model at its best: we compute metrics on the test set using the best threshold we found at the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_section)\n",
    "print(\"Testing starting with metrics:\")\n",
    "print(\", \".join(test))\n",
    "results = test_model(test_predictions, test_labels, best_threshold, concat, *test)\n",
    "summary_type = \"median\"\n",
    "\n",
    "if concat:\n",
    "    results_summary = np.transpose(results)\n",
    "    print(\"Results:\")\n",
    "else:\n",
    "    results_summary = np.transpose(summary_stats(results, type=summary_type))\n",
    "    print(f\"Summary statistics are based on the {summary_type}\")\n",
    "    print(\"Results (lower, mid-point, upper):\")\n",
    "print(f\"\\tBest threshold:\\t{best_threshold:.4f}\")\n",
    "for i, measure in enumerate(test):\n",
    "    print(\"\\t{}:\\t{}\".format(measure, results_summary[i, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the model wasn't loaded before, load it now\n",
    "try:\n",
    "    model\n",
    "except NameError:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(model_dir))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise predictions of the model on individual images\n",
    "vizualization_generator = show_full_comparisonTestGenerator(\n",
    "    model, threshold_prediction=best_threshold\n",
    ")\n",
    "images_to_show = 15\n",
    "count = 0\n",
    "for _ in vizualization_generator:\n",
    "    count += 1\n",
    "    if count >= image_to_show:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
